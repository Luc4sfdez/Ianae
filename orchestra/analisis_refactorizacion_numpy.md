# An√°lisis y Plan de Refactorizaci√≥n Numpy - IANAE nucleo.py

**Worker:** worker-core
**Fecha:** 2026-02-10
**Archivos Analizados:**
- `src/core/nucleo.py` (752 l√≠neas)
- `src/core/emergente.py` (882 l√≠neas)

---

## HALLAZGO IMPORTANTE

**El c√≥digo YA usa numpy extensivamente**. La migraci√≥n a numpy ya est√° implementada en gran medida. Sin embargo, se identificaron oportunidades significativas de optimizaci√≥n vectorial y mejoras de rendimiento.

---

## 1. AN√ÅLISIS DEL C√ìDIGO ACTUAL

### 1.1 Uso Actual de Numpy

**Operaciones que YA usan numpy:**
- ‚úÖ Vectores de conceptos: `np.array()` (l√≠neas 110, 213, 216, 219)
- ‚úÖ Normalizaci√≥n vectorial: `np.linalg.norm()` (l√≠nea 216)
- ‚úÖ Random normal: `np.random.normal()` (l√≠neas 213, 219, 532)
- ‚úÖ Producto punto: `np.dot()` (l√≠nea 254)
- ‚úÖ Random choice: `np.random.choice()` (l√≠nea 707)

### 1.2 Estructuras de Datos Identificadas

```python
# Concepto individual (l√≠neas 220-230)
self.conceptos[nombre] = {
    'base': np.array,          # ‚úÖ numpy array
    'actual': np.array,        # ‚úÖ numpy array
    'historial': [np.array],   # ‚ö†Ô∏è lista de numpy arrays
    'creado': int,
    'activaciones': int,
    'ultima_activacion': int,
    'fuerza': float,
    'categoria': str,
    'conexiones_proyecto': int
}

# Relaciones (l√≠nea 257)
self.relaciones[concepto1] = [(concepto2, fuerza), ...]  # ‚ö†Ô∏è lista de tuplas

# Activaciones (l√≠nea 513)
activacion = {c: 0.0 for c in self.conceptos}  # ‚ö†Ô∏è diccionario, no numpy
```

### 1.3 Cuellos de Botella Identificados

#### **Cuello #1: Propagaci√≥n de Activaci√≥n (l√≠neas 518-540)**
```python
for paso in range(pasos):
    nueva_activacion = activacion.copy()  # ‚ö†Ô∏è copia de diccionario

    for concepto, nivel in activacion.items():  # ‚ö†Ô∏è iteraci√≥n Python pura
        if nivel > 0.1:
            for vecino, fuerza in self.relaciones[concepto]:  # ‚ö†Ô∏è nested loops
                factor_aleatorio = np.random.uniform(1 - temperatura, 1 + temperatura)
                propagacion = nivel * fuerza * factor_aleatorio
                nueva_activacion[vecino] = max(nueva_activacion[vecino], propagacion)
```

**Problema:** Triple loop anidado con operaciones Python puras. Complejidad O(pasos √ó conceptos √ó relaciones_promedio).

**Estimaci√≥n:** Para 1000 conceptos con 10 relaciones cada uno, 3 pasos = ~30,000 operaciones.

#### **Cuello #2: Normalizaci√≥n Manual (l√≠neas 529-533)**
```python
total = sum(nueva_activacion.values()) + 1e-10  # ‚ö†Ô∏è sum() de diccionario
for c in nueva_activacion:
    nueva_activacion[c] = nueva_activacion[c] / total
    nueva_activacion[c] += np.random.normal(0, temperatura * 0.5)
    nueva_activacion[c] = max(0, min(1, nueva_activacion[c]))
```

**Problema:** Operaciones elemento por elemento en Python. No aprovecha vectorizaci√≥n numpy.

#### **Cuello #3: Auto-modificaci√≥n (l√≠neas 564-584)**
```python
for i in range(len(conceptos_activos)):
    for j in range(i+1, len(conceptos_activos)):  # ‚ö†Ô∏è O(n¬≤) loops
        c1, c2 = conceptos_activos[i], conceptos_activos[j]
        if self.grafo.has_edge(c1, c2):  # ‚ö†Ô∏è lookup individual
            # Actualizar relaciones uno por uno
            for idx, (vecino, peso) in enumerate(self.relaciones[c1]):
                if vecino == c2:
                    self.relaciones[c1][idx] = (vecino, nuevo_peso)
```

**Problema:** Actualizaci√≥n secuencial de relaciones. O(n¬≤) para n conceptos activos.

#### **Cuello #4: Relaciones como Listas de Tuplas**
```python
self.relaciones[concepto1].append((concepto2, fuerza))  # l√≠nea 257
```

**Problema:**
- B√∫squeda lineal O(n) para encontrar vecino espec√≠fico
- No hay operaciones vectoriales posibles
- Dif√≠cil de paralelizar

### 1.4 Operaciones Cr√≠ticas por Frecuencia

| Operaci√≥n | Frecuencia | Vectorizable | Impacto |
|-----------|------------|--------------|---------|
| Propagaci√≥n activaci√≥n | Alta (cada `activar()`) | ‚úÖ S√≠ | üî¥ **CR√çTICO** |
| Normalizaci√≥n | Alta (cada paso) | ‚úÖ S√≠ | üî¥ **CR√çTICO** |
| B√∫squeda vecinos | Alta (cada propagaci√≥n) | ‚úÖ S√≠ | üü° Alto |
| Auto-modificaci√≥n | Media (ciclos) | ‚ö†Ô∏è Parcial | üü° Alto |
| Guardar/cargar | Baja | ‚ùå No | üü¢ Bajo |

---

## 2. DISE√ëO DE LA MIGRACI√ìN

### 2.1 Matriz de Adyacencia para Relaciones

**Propuesta:** Reemplazar `self.relaciones` (dict de listas de tuplas) por matriz de adyacencia numpy.

**Estructura actual:**
```python
self.relaciones = {
    'Python': [('VBA', 0.9), ('OpenCV', 0.85), ...],
    'VBA': [('Python', 0.9), ('Excel', 0.95), ...],
    ...
}
```

**Estructura propuesta:**
```python
self.conceptos_idx = {'Python': 0, 'VBA': 1, 'OpenCV': 2, ...}  # mapeo nombre ‚Üí √≠ndice
self.matriz_relaciones = np.zeros((n_conceptos, n_conceptos), dtype=np.float32)
# matriz_relaciones[i, j] = fuerza de relaci√≥n entre concepto[i] y concepto[j]
```

**Ventajas:**
- ‚úÖ Lookup O(1) en vez de O(n)
- ‚úÖ Operaciones vectoriales (broadcasting, slicing)
- ‚úÖ Compatible con √°lgebra lineal
- ‚úÖ F√°cil de guardar/cargar (np.save/np.load)

**Trade-off:**
- ‚ö†Ô∏è Memoria: O(n¬≤) en vez de O(e) donde e = n√∫mero de edges
- Para 1000 conceptos: 1000¬≤ √ó 4 bytes = 4 MB (aceptable)
- Para 10000 conceptos: 10000¬≤ √ó 4 bytes = 400 MB (considerar sparse matrix)

### 2.2 Vector de Activaciones

**Propuesta:** Reemplazar diccionario por numpy array.

**Estructura actual:**
```python
activacion = {c: 0.0 for c in self.conceptos}  # diccionario
```

**Estructura propuesta:**
```python
activacion = np.zeros(n_conceptos, dtype=np.float32)
# activacion[idx] = nivel de activaci√≥n del concepto[idx]
```

**Ventajas:**
- ‚úÖ Operaciones vectoriales directas
- ‚úÖ Broadcasting autom√°tico
- ‚úÖ M√°s r√°pido (contiguo en memoria)

### 2.3 Propagaci√≥n Vectorizada

**Algoritmo actual (l√≠neas 518-540):**
```python
# O(pasos √ó conceptos √ó relaciones_promedio)
for paso in range(pasos):
    for concepto, nivel in activacion.items():
        if nivel > 0.1:
            for vecino, fuerza in self.relaciones[concepto]:
                propagacion = nivel * fuerza * factor
                nueva_activacion[vecino] = max(...)
```

**Algoritmo propuesto:**
```python
# O(pasos √ó conceptos) ‚Äî operaciones matriciales
for paso in range(pasos):
    # Vectorizar propagaci√≥n
    activos_mask = activacion > 0.1
    factor_aleatorio = np.random.uniform(
        1 - temperatura, 1 + temperatura,
        size=(n_conceptos, n_conceptos)
    )

    # Propagaci√≥n vectorizada (¬°UNA L√çNEA!)
    nueva_activacion = np.maximum(
        nueva_activacion,
        (activacion[:, np.newaxis] * self.matriz_relaciones * factor_aleatorio).max(axis=0)
    )

    # Normalizaci√≥n vectorizada
    nueva_activacion /= (nueva_activacion.sum() + 1e-10)
    nueva_activacion += np.random.normal(0, temperatura * 0.5, n_conceptos)
    nueva_activacion = np.clip(nueva_activacion, 0, 1)
```

**Mejora estimada:** 10-50x m√°s r√°pido (depende de n_conceptos).

### 2.4 Historial Optimizado

**Propuesta:** Usar numpy array 2D para historial de vectores.

**Estructura actual:**
```python
self.conceptos[nombre]['historial'] = [np.array1, np.array2, ...]  # lista
```

**Estructura propuesta:**
```python
self.conceptos[nombre]['historial'] = np.vstack([np.array1, np.array2, ...])
# Shape: (n_historiales, dim_vector)
```

**Ventajas:**
- ‚úÖ An√°lisis temporal vectorizado
- ‚úÖ C√°lculo de tendencias con np.mean(), np.std()
- ‚úÖ Detecci√≥n de drift con operaciones matriciales

---

## 3. PLAN DE IMPLEMENTACI√ìN

### Fase 1: Infraestructura Base (PRIORIDAD ALTA)
**Objetivo:** Preparar estructuras sin romper funcionalidad existente.

**Tareas:**
1. Crear mapeo bidireccional concepto ‚Üî √≠ndice
   ```python
   self.conceptos_idx = {}  # nombre ‚Üí √≠ndice
   self.idx_conceptos = []  # √≠ndice ‚Üí nombre
   ```

2. Inicializar matriz de relaciones
   ```python
   self.matriz_relaciones = np.zeros((max_conceptos, max_conceptos), dtype=np.float32)
   ```

3. Migrar relaciones existentes a matriz
   ```python
   def _migrar_relaciones_a_matriz(self):
       for c1, vecinos in self.relaciones.items():
           idx1 = self.conceptos_idx[c1]
           for c2, fuerza in vecinos:
               idx2 = self.conceptos_idx[c2]
               self.matriz_relaciones[idx1, idx2] = fuerza
   ```

4. Mantener compatibilidad con API existente (wrapper methods)

**Criterio de hecho:**
- ‚úÖ Tests existentes pasan sin cambios
- ‚úÖ Matriz de relaciones refleja self.relaciones
- ‚úÖ Rendimiento igual o mejor

**Tests necesarios:**
```python
def test_matriz_relaciones_equivalente():
    # Verificar que matriz y diccionario son equivalentes
    assert all(sistema.matriz_relaciones[i, j] == fuerza
               for c1, vecinos in sistema.relaciones.items()
               for c2, fuerza in vecinos)
```

### Fase 2: Refactorizar activar() (PRIORIDAD ALTA)
**Objetivo:** 2-10x mejora en propagaci√≥n.

**Tareas:**
1. Convertir activaci√≥n a vector numpy
   ```python
   activacion = np.zeros(len(self.conceptos), dtype=np.float32)
   activacion[self.conceptos_idx[concepto_inicial]] = 1.0
   ```

2. Implementar propagaci√≥n vectorizada (ver 2.3)

3. Agregar par√°metro legacy_mode para comparaci√≥n
   ```python
   def activar(self, concepto, pasos=3, temperatura=0.1, vectorizado=True):
       if vectorizado:
           return self._activar_vectorizado(...)
       else:
           return self._activar_legacy(...)  # mantener original
   ```

4. Benchmark comparativo

**Criterio de hecho:**
- ‚úÖ Resultados equivalentes (¬±0.01 por ruido estoc√°stico)
- ‚úÖ Mejora de rendimiento >2x
- ‚úÖ Tests pasan con vectorizado=True

**Tests necesarios:**
```python
def test_activar_vectorizado_equivalente():
    resultado_legacy = sistema.activar('Python', vectorizado=False)
    resultado_nuevo = sistema.activar('Python', vectorizado=True)
    assert np.allclose(resultado_legacy, resultado_nuevo, atol=0.05)

def test_activar_vectorizado_mas_rapido():
    import time
    t0 = time.time()
    sistema.activar('Python', pasos=10, vectorizado=False)
    t_legacy = time.time() - t0

    t0 = time.time()
    sistema.activar('Python', pasos=10, vectorizado=True)
    t_nuevo = time.time() - t0

    assert t_nuevo < t_legacy * 0.5  # Al menos 2x m√°s r√°pido
```

### Fase 3: Optimizar auto_modificar() (PRIORIDAD MEDIA)
**Objetivo:** Actualizaci√≥n batch de relaciones.

**Tareas:**
1. Identificar pares a reforzar en una sola pasada
2. Actualizar matriz con operaciones vectoriales
   ```python
   # En vez de loops anidados
   conceptos_activos_idx = np.where(activacion > 0.2)[0]
   pares = np.meshgrid(conceptos_activos_idx, conceptos_activos_idx)
   mask_pares = pares[0] < pares[1]  # solo upper triangle

   # Actualizar en batch
   self.matriz_relaciones[pares] = np.clip(
       self.matriz_relaciones[pares] + fuerza * np.random.random(pares.shape),
       0, 1
   )
   ```

**Criterio de hecho:**
- ‚úÖ Misma l√≥gica de refuerzo
- ‚úÖ M√°s r√°pido (3-5x)
- ‚úÖ Tests de regresi√≥n pasan

### Fase 4: Historial Vectorizado (PRIORIDAD BAJA)
**Objetivo:** An√°lisis temporal eficiente.

**Tareas:**
1. Migrar historial a np.array 2D
2. Implementar m√©tricas temporales vectorizadas
   ```python
   def analizar_drift_concepto(self, nombre):
       historial = self.conceptos[nombre]['historial']  # shape (n, dim)
       drift = np.diff(historial, axis=0)  # cambios temporales
       magnitud_drift = np.linalg.norm(drift, axis=1)
       return {
           'drift_promedio': magnitud_drift.mean(),
           'drift_max': magnitud_drift.max(),
           'estabilidad': 1.0 / (magnitud_drift.std() + 1e-10)
       }
   ```

**Criterio de hecho:**
- ‚úÖ An√°lisis temporal 5-10x m√°s r√°pido
- ‚úÖ Nuevas m√©tricas disponibles
- ‚úÖ Compatible con visualizaciones

### Fase 5: Sparse Matrix para Escalabilidad (FUTURO)
**Objetivo:** Escalar a 10,000+ conceptos.

**Condici√≥n:** Solo si n_conceptos > 5000 (matriz densa = 400MB+).

**Tareas:**
1. Usar scipy.sparse.csr_matrix para matriz de relaciones
2. Adaptar operaciones a formato sparse
3. Benchmark memoria vs velocidad

**Trade-off:**
- ‚úÖ Menos memoria (solo guardar edges no-cero)
- ‚ö†Ô∏è Operaciones sparse pueden ser m√°s lentas para matrices densas
- ‚ö†Ô∏è Mayor complejidad de c√≥digo

---

## 4. ESTIMACI√ìN DE IMPACTO

### 4.1 Rendimiento

| Operaci√≥n | Actual | Con Refactorizaci√≥n | Mejora |
|-----------|--------|---------------------|--------|
| Propagaci√≥n (n=100) | ~10 ms | ~1 ms | **10x** |
| Propagaci√≥n (n=1000) | ~200 ms | ~15 ms | **13x** |
| Auto-modificaci√≥n (n=100) | ~5 ms | ~1 ms | **5x** |
| Normalizaci√≥n | ~2 ms | ~0.1 ms | **20x** |
| Guardar estado | ~100 ms | ~50 ms | **2x** |

**Mejora global estimada:** 3-10x en operaciones cr√≠ticas, 2-3x en ciclo completo.

### 4.2 Memoria

| Estructura | Actual | Propuesta | Cambio |
|------------|--------|-----------|--------|
| 100 conceptos | ~50 KB | ~100 KB | +50 KB |
| 1000 conceptos | ~500 KB | ~4 MB | +3.5 MB |
| 10000 conceptos | ~5 MB | ~400 MB | +395 MB ‚ö†Ô∏è |

**Recomendaci√≥n:**
- Para n < 5000: matriz densa (√≥ptimo)
- Para n >= 5000: migrar a sparse matrix (Fase 5)

### 4.3 Escalabilidad

**L√≠mites actuales:**
- ~1000 conceptos antes de latencia notable
- ~5000 conceptos m√°ximo pr√°ctico

**L√≠mites con refactorizaci√≥n:**
- ~10,000 conceptos sin problemas (matriz densa)
- ~100,000+ conceptos posible (sparse matrix)

---

## 5. RIESGOS Y BREAKING CHANGES

### 5.1 Breaking Changes Identificados

#### **Cambio #1: API de activar() retorna numpy array**
**Antes:**
```python
resultado = sistema.activar('Python')  # retorna [dict, dict, dict]
resultado[-1]['Python']  # acceso por nombre
```

**Despu√©s:**
```python
resultado = sistema.activar('Python')  # retorna [np.array, np.array, np.array]
idx = sistema.conceptos_idx['Python']
resultado[-1][idx]  # acceso por √≠ndice
```

**Mitigaci√≥n:**
- Mantener modo legacy con par√°metro
- Crear helper: `sistema.get_activacion(resultado, nombre_concepto)`

#### **Cambio #2: Orden de conceptos importa**
Con matriz, el orden de conceptos est√° fijo por √≠ndices. A√±adir concepto nuevo requiere:
- Redimensionar matriz (costoso)
- O reservar espacio (waste de memoria)

**Mitigaci√≥n:**
- Pre-allocar matriz para max_conceptos (ej: 10,000)
- Lazy initialization: solo crecer cuando necesario

#### **Cambio #3: Serializaci√≥n diferente**
Guardar/cargar debe manejar matrices numpy.

**Antes:**
```python
json.dump(self.relaciones, f)  # dict serializable
```

**Despu√©s:**
```python
np.save(f, self.matriz_relaciones)  # formato numpy
```

**Mitigaci√≥n:**
- Mantener compatibilidad con formato antiguo
- Detectar formato autom√°ticamente al cargar

### 5.2 Riesgos

| Riesgo | Probabilidad | Impacto | Mitigaci√≥n |
|--------|--------------|---------|------------|
| Tests fallan por diferencias num√©ricas | Alta | Medio | Tolerancia ¬±0.05 |
| Regresi√≥n de rendimiento en casos peque√±os | Media | Bajo | Benchmark continuo |
| Uso excesivo de memoria | Baja | Alto | Sparse matrix (Fase 5) |
| Breaking API externa | Media | Alto | Mantener wrappers legacy |

---

## 6. ESTRATEGIA DE TESTING

### 6.1 Tests Unitarios Nuevos

```python
# test_nucleo_numpy.py

def test_matriz_relaciones_equivalencia():
    """Verificar matriz equivale a dict de relaciones"""
    ...

def test_propagacion_vectorizada_determinista():
    """Con seed fija, resultados deben ser reproducibles"""
    np.random.seed(42)
    ...

def test_activar_vectorizado_vs_legacy():
    """Comparar resultados con m√©todo original"""
    ...

def test_rendimiento_propagacion():
    """Benchmark: vectorizado debe ser >2x m√°s r√°pido"""
    ...

def test_auto_modificacion_batch():
    """Verificar actualizaci√≥n batch de relaciones"""
    ...

def test_memoria_matriz_vs_dict():
    """Comparar uso de memoria"""
    ...

def test_serializaci√≥n_matriz():
    """Guardar y cargar matriz"""
    ...
```

### 6.2 Tests de Regresi√≥n

Ejecutar TODOS los tests existentes:
```bash
pytest tests/test_nucleo.py -v
pytest tests/test_emergente.py -v
```

Todos deben pasar sin modificaciones.

### 6.3 Benchmarks

```python
# benchmark_refactorizacion.py

import time
import numpy as np

def benchmark_activar(sistema, n_repeticiones=100):
    tiempos_legacy = []
    tiempos_nuevo = []

    for _ in range(n_repeticiones):
        t0 = time.time()
        sistema.activar('Python', vectorizado=False)
        tiempos_legacy.append(time.time() - t0)

        t0 = time.time()
        sistema.activar('Python', vectorizado=True)
        tiempos_nuevo.append(time.time() - t0)

    print(f"Legacy: {np.mean(tiempos_legacy)*1000:.2f}ms ¬± {np.std(tiempos_legacy)*1000:.2f}ms")
    print(f"Nuevo:  {np.mean(tiempos_nuevo)*1000:.2f}ms ¬± {np.std(tiempos_nuevo)*1000:.2f}ms")
    print(f"Speedup: {np.mean(tiempos_legacy)/np.mean(tiempos_nuevo):.2f}x")
```

---

## 7. DEPENDENCIAS

### 7.1 Ya Instaladas
- ‚úÖ numpy>=1.20.0 (requirements.txt l√≠nea 1)
- ‚úÖ matplotlib>=3.5.0 (visualizaci√≥n)
- ‚úÖ networkx>=2.8.0 (grafo)

### 7.2 Opcionales (Fase 5)
- scipy>=1.9.0 (sparse matrices)
- numba>=0.56.0 (JIT compilation para loops inevitables)

---

## 8. RESUMEN EJECUTIVO

### Estado Actual
- ‚úÖ nucleo.py YA usa numpy para vectores
- ‚ö†Ô∏è Operaciones de propagaci√≥n SIN vectorizar (bucles Python)
- ‚ö†Ô∏è Relaciones como dict de listas (no vectorizable)

### Oportunidades de Optimizaci√≥n
1. **Matriz de adyacencia** ‚Üí 10-50x m√°s r√°pido en propagaci√≥n
2. **Propagaci√≥n vectorizada** ‚Üí eliminar triple loop anidado
3. **Vectorizaci√≥n de activaciones** ‚Üí broadcasting autom√°tico
4. **Batch updates** en auto-modificaci√≥n ‚Üí 5x m√°s r√°pido

### Impacto Esperado
- üöÄ **3-10x mejora** en operaciones cr√≠ticas
- üìà **Escalabilidad** a 10,000+ conceptos
- üíæ **+4 MB** memoria (1000 conceptos) ‚Äî aceptable

### Riesgos Controlados
- API compatible con wrappers legacy
- Tests de regresi√≥n completos
- Migraci√≥n incremental por fases

---

## 9. PR√ìXIMOS PASOS RECOMENDADOS

### Paso 1: Aprobar Plan
Lucas revisa y aprueba este an√°lisis.

### Paso 2: Implementar Fase 1
Worker-core implementa infraestructura base (matriz de relaciones).

**Tiempo estimado:** 2-4 horas
**Archivos afectados:** src/core/nucleo.py (a√±adir m√©todos, no modificar existentes)

### Paso 3: Tests y Benchmark
Worker-infra crea tests unitarios y benchmarks.

**Tiempo estimado:** 1-2 horas
**Archivos nuevos:** tests/test_nucleo_numpy.py, tests/benchmark_refactorizacion.py

### Paso 4: Implementar Fase 2
Worker-core refactoriza activar() con vectorizaci√≥n.

**Tiempo estimado:** 3-5 horas
**Archivos afectados:** src/core/nucleo.py (m√©todo activar)

### Paso 5: Validaci√≥n
Ejecutar todos los tests, benchmarks, y experimentos de validaci√≥n.

**Tiempo estimado:** 1 hora

### Paso 6: Fases 3-4 (opcional, seg√∫n resultados)
Continuar con auto_modificar() e historial si Fases 1-2 exitosas.

---

## 10. CONCLUSI√ìN

El c√≥digo actual de IANAE usa numpy para vectores individuales pero NO aprovecha vectorizaci√≥n para operaciones batch. La refactorizaci√≥n propuesta puede lograr **3-10x mejora de rendimiento** con riesgo controlado y sin breaking changes (mediante wrappers).

**Recomendaci√≥n:** APROBAR e implementar en fases incrementales, comenzando con matriz de relaciones (Fase 1) y propagaci√≥n vectorizada (Fase 2).

**Criterio de √©xito global:**
- ‚úÖ Mejora de rendimiento >2x en propagaci√≥n
- ‚úÖ Todos los tests existentes pasan
- ‚úÖ Escalabilidad a 5000+ conceptos
- ‚úÖ Sin breaking changes en API p√∫blica

---

**Reporte generado por:** worker-core
**Sistema:** claude-orchestra
**Arquitecto:** Daemon IA

